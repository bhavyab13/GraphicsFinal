{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook implements facial landmark detection, lip segmentation, and lighting estimation for lipstick rendering."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import mediapipe as mp\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import json\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Load CelebA Metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "path  CelebAMask-HQ\\CelebAMask-HQ\\CelebA-HQ-img exists  True\n"
     ]
    }
   ],
   "source": [
    "def load_celeba_data(image_dir = \"CelebAMask-HQ\\CelebAMask-HQ\\CelebA-HQ-img\"):\n",
    "    # Assuming images are stored in 'img_align_celeba' directory relative to metadata file\n",
    "    image_files = []\n",
    "    print(\"path \",image_dir + \" exists \",os.path.exists(image_dir))\n",
    "    if os.path.exists(image_dir):\n",
    "        image_files = [os.path.join(image_dir, f) for f in os.listdir(image_dir) if f.endswith('.jpg')]\n",
    "    return image_files\n",
    "\n",
    "\n",
    "files = load_celeba_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Image Processing Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_image(image_path):\n",
    "    image = cv2.imread(image_path)\n",
    "    image_resized = cv2.resize(image, (512, 512))\n",
    "    image_rgb = cv2.cvtColor(image_resized, cv2.COLOR_BGR2RGB)\n",
    "    return image_rgb\n",
    "\n",
    "def lip_segmentation(image_rgb):\n",
    "    # mp_face_mesh = mp.solutions.face_mesh\n",
    "    face_mesh = mp.solutions.face_mesh.FaceMesh(\n",
    "        static_image_mode=True, max_num_faces=1, refine_landmarks=True)\n",
    "    results = face_mesh.process(image_rgb)\n",
    "    \n",
    "    lip_indices = [\n",
    "        61, 146, 91, 181, 84, 17, 314, 405, 321, 375,\n",
    "        291, 61, 185, 40, 39, 37, 0, 267, 269, 270,\n",
    "        409, 291, 78, 95, 88, 178, 87, 14, 317, 402,\n",
    "        318, 324, 308, 78, 191, 80, 81, 82, 13, 312,\n",
    "        311, 310, 415, 308\n",
    "    ]\n",
    "    \n",
    "    if results.multi_face_landmarks:\n",
    "        face_landmarks = results.multi_face_landmarks[0]\n",
    "        lip_landmarks = [face_landmarks.landmark[i] for i in lip_indices]\n",
    "        mask = np.zeros(image_rgb.shape[:2], dtype=np.uint8)\n",
    "        points = [\n",
    "            (int(landmark.x * image_rgb.shape[1]), int(landmark.y * image_rgb.shape[0]))\n",
    "            for landmark in lip_landmarks\n",
    "        ]\n",
    "        points = np.array(points, dtype=np.int32)\n",
    "        cv2.fillPoly(mask, [points], 255)\n",
    "        lip_mask = cv2.bitwise_and(image_rgb, image_rgb, mask=mask)\n",
    "        return lip_mask, mask\n",
    "    return None, None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Lighting Estimation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "def evaluate_sh_basis_gpu(theta, phi):\n",
    "    \"\"\"Vectorized SH basis computation on GPU\"\"\"\n",
    "    sh_basis = torch.zeros((theta.shape[0], 9), device=theta.device)\n",
    "    \n",
    "    # Y00\n",
    "    sh_basis[:, 0] = 0.282095 * torch.ones_like(theta)\n",
    "    # Y1m\n",
    "    sh_basis[:, 1] = 0.488603 * torch.sin(theta) * torch.cos(phi)\n",
    "    sh_basis[:, 2] = 0.488603 * torch.sin(theta) * torch.sin(phi)\n",
    "    sh_basis[:, 3] = 0.488603 * torch.cos(theta)\n",
    "    # Y2m\n",
    "    sh_basis[:, 4] = 1.092548 * torch.sin(theta)**2 * torch.cos(2*phi)\n",
    "    sh_basis[:, 5] = 1.092548 * torch.sin(theta) * torch.cos(theta) * torch.cos(phi)\n",
    "    sh_basis[:, 6] = 0.315392 * (3*torch.cos(theta)**2 - 1)\n",
    "    sh_basis[:, 7] = 1.092548 * torch.sin(theta) * torch.cos(theta) * torch.sin(phi)\n",
    "    sh_basis[:, 8] = 0.546274 * torch.sin(theta)**2 * torch.cos(2*phi)\n",
    "    \n",
    "    return sh_basis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "def estimate_lighting_gpu(image_rgb, face_landmarks, device='cuda'):\n",
    "    \"\"\"GPU-accelerated lighting estimation using spherical harmonics\"\"\"\n",
    "    # Convert inputs to torch tensors with explicit dtype\n",
    "    image_tensor = torch.from_numpy(image_rgb).float().to(device)\n",
    "    \n",
    "    # Extract face geometry\n",
    "    image_h, image_w = image_rgb.shape[:2]\n",
    "    vertices = torch.tensor([\n",
    "        [landmark.x * image_w, landmark.y * image_h, landmark.z]\n",
    "        for landmark in face_landmarks.landmark\n",
    "    ], dtype=torch.float32, device=device)\n",
    "    \n",
    "    # Compute face normals using neighboring vertices\n",
    "    v1 = vertices[1:-1] - vertices[:-2]  # Size: N-2\n",
    "    v2 = vertices[2:] - vertices[1:-1]   # Size: N-2\n",
    "    \n",
    "    # Debug size check\n",
    "    assert v1.shape == v2.shape, f\"Shape mismatch: v1 {v1.shape} vs v2 {v2.shape}\"\n",
    "    \n",
    "    normals = torch.cross(v1, v2)\n",
    "    normals = normals / (torch.norm(normals, dim=1, keepdim=True) + 1e-6)  # Add epsilon to avoid div by 0\n",
    "    \n",
    "    # Create face mask\n",
    "    face_mask = torch.zeros((image_h, image_w), dtype=torch.float32, device=device)\n",
    "    points = vertices[:, :2].cpu().numpy().astype(np.int32)\n",
    "    face_mask = torch.from_numpy(\n",
    "        cv2.fillConvexPoly(face_mask.cpu().numpy(), points, 1)\n",
    "    ).to(device)\n",
    "    \n",
    "    # Sample face colors and positions\n",
    "    y_coords, x_coords = torch.where(face_mask > 0)\n",
    "    colors = image_tensor[y_coords, x_coords].float()  # Ensure float type\n",
    "    \n",
    "    # Convert to spherical coordinates\n",
    "    positions = torch.stack([\n",
    "        x_coords.float() / image_w * 2 - 1,\n",
    "        y_coords.float() / image_h * 2 - 1\n",
    "    ], dim=1)\n",
    "    \n",
    "    # Compute spherical harmonics basis\n",
    "    theta = torch.arccos(torch.clamp(positions[:, 1], -1, 1))\n",
    "    phi = torch.arctan2(positions[:, 0], torch.ones_like(positions[:, 0]))\n",
    "    \n",
    "    # Evaluate SH basis functions\n",
    "    sh_basis = evaluate_sh_basis_gpu(theta, phi)\n",
    "    \n",
    "    # Debug dtype check\n",
    "    assert sh_basis.dtype == colors.dtype, f\"Dtype mismatch: sh_basis {sh_basis.dtype} vs colors {colors.dtype}\"\n",
    "    \n",
    "    # Solve for coefficients using least squares\n",
    "    sh_coefficients = torch.linalg.lstsq(sh_basis, colors).solution[:9]\n",
    "    \n",
    "    return sh_coefficients.cpu().numpy()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Physically-Based Lipstick Rendering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from OpenGL.GL import *\n",
    "from OpenGL.GL.shaders import compileShader, compileProgram\n",
    "import glfw\n",
    "\n",
    "# Load and compile GLSL shader from file\n",
    "def load_shader(shader_path, shader_type):\n",
    "    with open(shader_path, 'r') as file:\n",
    "        shader_code = file.read()\n",
    "    shader = compileShader(shader_code, shader_type)\n",
    "    return shader\n",
    "\n",
    "# Compile shaders into OpenGL program\n",
    "def create_shader_program(vertex_shader_path, fragment_shader_path):\n",
    "    vertex_shader = load_shader(vertex_shader_path, GL_VERTEX_SHADER)\n",
    "    fragment_shader = load_shader(fragment_shader_path, GL_FRAGMENT_SHADER)\n",
    "    program = compileProgram(vertex_shader, fragment_shader)\n",
    "    return program\n",
    "\n",
    "# Shader file paths\n",
    "vertex_shader_path = os.path.join('shaders', 'vertex_shader.glsl')\n",
    "fragment_shader_path = os.path.join('shaders', 'fragment_shader.glsl')\n",
    "\n",
    "# Integrate with the Main Pipeline\n",
    "def render_lip_texture(image_rgb, sh_coefficients, base_color, specular_color, glossiness):\n",
    "    # Initialize GLFW for creating an OpenGL context\n",
    "    if not glfw.init():\n",
    "        raise Exception(\"GLFW could not be initialized\")\n",
    "    \n",
    "    # Create a window (hidden, since this is for computation only)\n",
    "    window = glfw.create_window(512, 512, \"Lip Renderer\", None, None)\n",
    "    glfw.make_context_current(window)\n",
    "\n",
    "    # Compile and link shaders\n",
    "    shader_program = create_shader_program(vertex_shader_path, fragment_shader_path)\n",
    "    glUseProgram(shader_program)\n",
    "\n",
    "    # Set uniform values\n",
    "    for i in range(9):\n",
    "        glUniform3fv(glGetUniformLocation(shader_program, f\"shCoefficients[{i}]\"), 1, sh_coefficients[i])\n",
    "        glUniform3fv(glGetUniformLocation(shader_program, \"baseColor\"), 1, base_color)\n",
    "        glUniform3fv(glGetUniformLocation(shader_program, \"specularColor\"), 1, specular_color)\n",
    "        glUniform1f(glGetUniformLocation(shader_program, \"glossiness\"), glossiness)\n",
    "        glUniform3f(glGetUniformLocation(shader_program, \"viewPos\"), 0.0, 0.0, 3.0)\n",
    "\n",
    "    # Render to texture\n",
    "    framebuffer = glGenFramebuffers(1)\n",
    "    texture = glGenTextures(1)\n",
    "    glBindTexture(GL_TEXTURE_2D, texture)\n",
    "    glTexImage2D(GL_TEXTURE_2D, 0, GL_RGB, 512, 512, 0, GL_RGB, GL_UNSIGNED_BYTE, None)\n",
    "    glFramebufferTexture2D(GL_FRAMEBUFFER, GL_COLOR_ATTACHMENT0, GL_TEXTURE_2D, texture, 0)\n",
    "\n",
    "    glBindFramebuffer(GL_FRAMEBUFFER, framebuffer)\n",
    "    glClear(GL_COLOR_BUFFER_BIT | GL_DEPTH_BUFFER_BIT)\n",
    "\n",
    "    result = glReadPixels(0, 0, 512, 512, GL_RGB, GL_UNSIGNED_BYTE)\n",
    "    result_image = np.frombuffer(result, dtype=np.uint8).reshape(512, 512, 3)\n",
    "\n",
    "    # Clean up\n",
    "    glBindFramebuffer(GL_FRAMEBUFFER, 0)\n",
    "    glDeleteFramebuffers(1, [framebuffer])\n",
    "    glfw.terminate()\n",
    "\n",
    "    return result_image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Main Processing Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "path  CelebAMask-HQ\\CelebAMask-HQ\\CelebA-HQ-img exists  True\n",
      "Processed and saved outputs for 0\n",
      "Processed and saved outputs for 1\n",
      "Processed and saved outputs for 10\n",
      "Processed and saved outputs for 100\n",
      "Processed and saved outputs for 1000\n",
      "Processed and saved outputs for 10000\n",
      "Processed and saved outputs for 10001\n",
      "Processed and saved outputs for 10002\n",
      "Processed and saved outputs for 10003\n",
      "Processed and saved outputs for 10004\n",
      "Processed and saved outputs for 10005\n",
      "Processed and saved outputs for 10006\n",
      "Processed and saved outputs for 10007\n",
      "Processed and saved outputs for 10008\n",
      "Processed and saved outputs for 10009\n",
      "Processed and saved outputs for 1001\n",
      "Processed and saved outputs for 10010\n",
      "Processed and saved outputs for 10011\n",
      "Processed and saved outputs for 10012\n",
      "Processed and saved outputs for 10013\n",
      "Processed and saved outputs for 10014\n",
      "Processed and saved outputs for 10015\n",
      "Processed and saved outputs for 10016\n",
      "Processed and saved outputs for 10017\n",
      "Processed and saved outputs for 10018\n",
      "Processed and saved outputs for 10019\n",
      "Processed and saved outputs for 1002\n",
      "Processed and saved outputs for 10020\n",
      "Processed and saved outputs for 10021\n",
      "Processed and saved outputs for 10022\n",
      "Processed and saved outputs for 10023\n",
      "Processed and saved outputs for 10024\n",
      "Processed and saved outputs for 10025\n",
      "Processed and saved outputs for 10026\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[24], line 16\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m image_path \u001b[38;5;129;01min\u001b[39;00m image_files:\n\u001b[0;32m     13\u001b[0m     \u001b[38;5;66;03m# Get base filename without extension\u001b[39;00m\n\u001b[0;32m     14\u001b[0m     base_name \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39msplitext(os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mbasename(image_path))[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m---> 16\u001b[0m     image_rgb \u001b[38;5;241m=\u001b[39m \u001b[43mpreprocess_image\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimage_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     17\u001b[0m     lip_mask, lip_binary_mask \u001b[38;5;241m=\u001b[39m lip_segmentation(image_rgb)\n\u001b[0;32m     19\u001b[0m     mp_face_mesh \u001b[38;5;241m=\u001b[39m mp\u001b[38;5;241m.\u001b[39msolutions\u001b[38;5;241m.\u001b[39mface_mesh\n",
      "Cell \u001b[1;32mIn[15], line 2\u001b[0m, in \u001b[0;36mpreprocess_image\u001b[1;34m(image_path)\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpreprocess_image\u001b[39m(image_path):\n\u001b[1;32m----> 2\u001b[0m     image \u001b[38;5;241m=\u001b[39m \u001b[43mcv2\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mimread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimage_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      3\u001b[0m     image_resized \u001b[38;5;241m=\u001b[39m cv2\u001b[38;5;241m.\u001b[39mresize(image, (\u001b[38;5;241m512\u001b[39m, \u001b[38;5;241m512\u001b[39m))\n\u001b[0;32m      4\u001b[0m     image_rgb \u001b[38;5;241m=\u001b[39m cv2\u001b[38;5;241m.\u001b[39mcvtColor(image_resized, cv2\u001b[38;5;241m.\u001b[39mCOLOR_BGR2RGB)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# Load CelebA Metadata\n",
    "# metadata_path = 'celeba-dataset-metadata.json'\n",
    "image_files = load_celeba_data(image_dir=\"CelebAMask-HQ\\CelebAMask-HQ\\CelebA-HQ-img\") # change this on different devices\n",
    "\n",
    "# Create output directory if it doesn't exist\n",
    "output_dir = 'lip-segmentation-outputs'\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# Process Each Image\n",
    "for image_path in image_files:\n",
    "    # Get base filename without extension\n",
    "    base_name = os.path.splitext(os.path.basename(image_path))[0]\n",
    "    \n",
    "    image_rgb = preprocess_image(image_path)\n",
    "    lip_mask, lip_binary_mask = lip_segmentation(image_rgb)\n",
    "    \n",
    "    mp_face_mesh = mp.solutions.face_mesh\n",
    "    with mp_face_mesh.FaceMesh(\n",
    "        static_image_mode=True, max_num_faces=1, refine_landmarks=True) as face_mesh:\n",
    "        results = face_mesh.process(image_rgb)\n",
    "        if results.multi_face_landmarks:\n",
    "            face_landmarks = results.multi_face_landmarks[0]\n",
    "            sh_coefficients = estimate_lighting_gpu(image_rgb, face_landmarks)\n",
    "\n",
    "            # Render lip BRDF (change the following constants)\n",
    "            base_color = np.array([1.0, 0.3, 0.4], dtype=np.float32)  # Lip base color\n",
    "            specular_color = np.array([1.0, 1.0, 1.0], dtype=np.float32)\n",
    "            glossiness = 32.0\n",
    "            lip_texture_img = render_lip_texture(image_rgb, sh_coefficients, base_color, specular_color, glossiness)\n",
    "            plt.imshow(lip_texture_img)\n",
    "            \n",
    "            # Save outputs\n",
    "            output_path = os.path.join(output_dir, f'{base_name}')\n",
    "            cv2.imwrite(f'{output_path}_lip_mask.png', cv2.cvtColor(lip_mask, cv2.COLOR_RGB2BGR))\n",
    "            cv2.imwrite(f'{output_path}_binary_mask.png', lip_binary_mask)\n",
    "            np.save(f'{output_path}_lighting.npy', sh_coefficients)\n",
    "            \n",
    "            print(f'Processed and saved outputs for {base_name}')\n",
    "        else:\n",
    "            print(f'No face detected in {image_path}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Next Step - Parallelize the processing with a dataloader type multiprocessing. Perhaps using image transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
