{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook implements facial landmark detection, lip segmentation, and lighting estimation for lipstick rendering."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"OPENCV_IO_ENABLE_OPENEXR\"]=\"1\"\n",
    "import cv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mediapipe as mp\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Load CelebA Metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "path  test_publishable_list exists  True\n"
     ]
    }
   ],
   "source": [
    "def load_celeba_data(image_dir = \"test_publishable_list\"):\n",
    "    # Assuming images are stored in 'test_publishable_list' directory relative to metadata file\n",
    "    image_files = []\n",
    "    print(\"path \",image_dir + \" exists \",os.path.exists(image_dir))\n",
    "    if os.path.exists(image_dir):\n",
    "        image_files = [os.path.join(image_dir, f) for f in os.listdir(image_dir) if f.startswith('test_publishable_list')]\n",
    "    # if os.path.exists(image_dir):\n",
    "    #     for f in os.listdir(image_dir):\n",
    "    #         if f.startswith('test_publishable_list'):\n",
    "    #             exr_path = os.path.join(image_dir, f, '1_Albedo.exr')\n",
    "    #             if os.path.exists(exr_path):\n",
    "    #                 image_files.append(exr_path)\n",
    "    return image_files\n",
    "\n",
    "\n",
    "files = load_celeba_data(\"test_publishable_list\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Image Processing Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "path  test_publishable_list exists  True\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1733641394.778992  280999 gl_context.cc:357] GL version: 2.1 (2.1 Metal - 88.1), renderer: Apple M1\n",
      "INFO: Created TensorFlow Lite XNNPACK delegate for CPU.\n",
      "W0000 00:00:1733641394.781707  281257 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1733641394.787567  281255 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1733641394.789911  281257 landmark_projection_calculator.cc:186] Using NORM_RECT without IMAGE_DIMENSIONS is only supported for the square ROI. Provide IMAGE_DIMENSIONS or use PROJECTION_MATRIX.\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAa4AAAGzCAYAAAB3vfPfAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjkuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8hTgPZAAAACXBIWXMAAA9hAAAPYQGoP6dpAAA6nUlEQVR4nO3dfZQU1Z0//ve9VdU9T/QMDDADkVGMD4QAJqJibx7WlYmIo5v4kBjlJMTkJAeFxKfDWUm+Ac3JBjc52U2MCdldT0LOHqMRI9lEhTiC4tMEAWVFNEQSzPBTZoYHp4eH6e6qez+/P6qmtSMaB4YZLrxfnhtnqm533y44vnOrPnVLiYiAiIjIEXqoB0BERNQfDC4iInIKg4uIiJzC4CIiIqcwuIiIyCkMLiIicgqDi4iInMLgIiIipzC4iIjIKQwuon569dVXoZTC0qVLh3ooA+oLX/gCampqhnoYRH8Xg4voLZYuXQqlFNavXz+on3veeedBKYVTTz31oPtbW1uhlIJSCvfff/+gjo3oaOMP9QCIXHPiiSeit7cXQRAM6PtWVFRg69atePbZZ3HOOeeU7bv77rtRUVGBfD4/oJ9J5CLOuIj6SSmFiooKeJ43oO/7/ve/H6effjruueeesu35fB7Lly9HS0vLgH4ekasYXET9dLBrXH3Xh/7yl79gxowZqK6uxtixY/Gtb30L/XkAw1VXXYVf/epXsNaWtv3ud7/DgQMH8JnPfOZt/f/617/iuuuuw+mnn47KykrU19fj05/+NF599dWyfmEY4rbbbsOpp56KiooK1NfX46Mf/ShaW1vfdTwbN27EqFGjcN5552Hfvn3v+XsQHUkMLqIBYozBhRdeiIaGBnz3u9/F1KlTsWjRIixatOg9v8fVV1+NHTt24PHHHy9t++Uvf4np06dj9OjRb+u/bt06PPPMM/jsZz+LO+64A3PmzMGqVatw3nnn4cCBA6V+t956K2677Tb80z/9E+6880584xvfQFNTE5577rl3HMu6detw/vnn48Mf/jBWrFjBwg06avAaF9EAyefzuPDCC3HHHXcAAK677jpccskl+Ld/+zd87Wtfw8iRI//ue5x66qk466yz8Mtf/hLnn38+uru78fDDD+O///u/D9q/paUFV1xxRdm2Sy65BNlsFr/+9a/xuc99DgDw0EMP4aKLLsJ//dd/vafv8vTTT+Oiiy7Cxz72Mfz6179GOp1+T68jGgyccRENoHnz5pV+Vkph3rx5KBaLePTRR9/ze1x99dV44IEHUCwWcf/998PzPFx66aUH7VtZWVn6OQxD7N69G6eccgrq6urKZlN1dXXYvHkzXnnllb/7+Y899hhmzJiB6dOn44EHHmBo0VGHwUU0QLTWOPnkk8u2nXbaaQDwtmtO7+azn/0scrkcVqxYgbvvvhsXX3wxhg0bdtC+vb29WLhwIcaNG4d0Oo2RI0di1KhR6O7uRi6XK/X71re+he7ubpx22mmYPHky5s+fjxdeeOFt75fP59HS0oIPf/jDuO+++5BKpd7zuIkGC4OL6CgzZswYnHfeefj+97+PJ554AldfffU79v3qV7+Kf/3Xf8VnPvMZ3HfffXjkkUfQ2tqK+vr6sgKPj3/84/jzn/+Mn/3sZ5g0aRLuuusunHnmmbjrrrvK3i+dTqOlpQVr167FypUrj9h3JDocDC6iAWKtxV/+8peybX/6058AACeddFK/3uvqq6/Gk08+iUwmg4suuugd+91///2YPXs2vv/97+OKK67AJz7xCXz0ox9Fd3f32/qOGDEC11xzDe655x5s374dU6ZMwa233lrWRymFu+++G9OnT8enP/3psiIRoqMFg4toAN15552ln0UEd955J4IgwPTp0/v1PldccQUWLVqEn/zkJ+96us7zvLeV2//oRz+CMaZs2+7du8t+r6mpwSmnnIJCofC290ylUnjggQdw9tln45JLLsGzzz7br7ETHWmsKiQ6iJ/97GcHPVV2/fXXv+NrKioqsHLlSsyePRvTpk3DihUr8NBDD+HrX/86Ro0a1a/Pr62tfdts6GAuvvhi/M///A9qa2sxceJEtLW14dFHH0V9fX1Zv4kTJ+K8887D1KlTMWLECKxfvx73339/WTHJW1VWVuLBBx/E+eefj5kzZ2LNmjWYNGlSv74D0ZHC4CI6iCVLlhx0+xe+8IV3fI3neVi5ciWuvfZazJ8/H8OGDcOiRYuwcOHCIzRK4Ic//CE8z8Pdd9+NfD6Pj3zkI3j00UcxY8aMsn5f+9rX8Nvf/haPPPIICoUCTjzxRHz729/G/Pnz3/G9M5kMfv/73+PjH/84PvGJT+DJJ5/EKaeccsS+C9F7paQ/t/UT0UF94QtfwP3338/VJYgGAa9xERGRUxhcRETkFAYXERE5ZciC68c//jFOOukkVFRUYNq0aSy5JactXbqU17eIBsmQBNevfvUr3HTTTVi0aBGee+45nHHGGZgxYwa6urqGYjhEROSQIakqnDZtGs4+++zSzZrWWowbNw5f/epXccsttwz2cIiIyCGDfh9XsVjEhg0bsGDBgtI2rTWam5vR1tZ20NcUCoWyO/yttdizZw/q6+uhlDriYyYiooElIti7dy/Gjh0Lrft38m/Qg2vXrl0wxqChoaFse0NDA/74xz8e9DWLFy/GbbfdNhjDIyKiQbR9+3accMIJ/XqNE1WFCxYsQC6XK7X29vahHhIREQ2Ad3pkz7sZ9BnXyJEj4XkeOjs7y7Z3dnaisbHxoK9Jp9N8mB0R0THoUC73DPqMK5VKYerUqVi1alVpm7UWq1atQjabHezhEBGRY4Zkkd2bbroJs2fPxllnnYVzzjkHP/jBD7B//35cc801QzEcIiJyyJAE15VXXomdO3di4cKF6OjowIc+9CGsXLnybQUbREREf8vJ1eF7enpQW1s71MMgIqLDlMvlkMlk+vUaJ6oKiYiI+jC4iIjIKQwuIiJyCoOLiIicwuAiIiKnMLiIiMgpDC4iInIKg4uIiJzC4CIiIqcwuIiIyCkMLiIicgqDi4iInMLgIiIipzC4iIjIKQwuIiJyCoOLiIicwuAiIiKnMLiIiMgpDC4iInIKg4uIiJzC4CIiIqcwuIiIyCkMLiIicgqDi4iInMLgIiIipzC4iIjIKQwuIiJyCoOLiIicwuAiIiKnMLiIiMgpDC4iInIKg4uIiJzC4CIiIqcwuIiIyCkMLiIicgqDi4iInMLgIiIipzC4iIjIKQwuIiJyCoOLiIicwuAiIiKnMLiIiMgpDC4iInIKg4uIiJzC4CIiIqcwuIiIyCkMLiIicgqDi4iInMLgIiIipzC4iIjIKQwuIiJyCoOLiIicwuAiIiKnMLiIiMgpDC4iInIKg4uIiJzC4CIiIqcwuIiIyCkMLiIicgqDi4iInMLgIiIip/Q7uJ544glccsklGDt2LJRS+M1vflO2X0SwcOFCjBkzBpWVlWhubsYrr7xS1mfPnj2YNWsWMpkM6urq8KUvfQn79u07rC9CRETHh34H1/79+3HGGWfgxz/+8UH3f/e738Udd9yBn/70p1i7di2qq6sxY8YM5PP5Up9Zs2Zh8+bNaG1txYMPPognnngCX/nKVw79WxAR0fFDDgMAWb58eel3a600NjbK9773vdK27u5uSafTcs8994iIyEsvvSQAZN26daU+K1asEKWUvPbaa+/pc3O5nABgY2NjY3O85XK5fmfPgF7j2rZtGzo6OtDc3FzaVltbi2nTpqGtrQ0A0NbWhrq6Opx11lmlPs3NzdBaY+3atQd930KhgJ6enrJGRETHpwENro6ODgBAQ0ND2faGhobSvo6ODowePbpsv+/7GDFiRKnP31q8eDFqa2tLbdy4cQM5bCIicogTVYULFixALpcrte3btw/1kIiIaIgMaHA1NjYCADo7O8u2d3Z2lvY1Njaiq6urbH8URdizZ0+pz99Kp9PIZDJljYiIjk8DGlzjx49HY2MjVq1aVdrW09ODtWvXIpvNAgCy2Sy6u7uxYcOGUp/Vq1fDWotp06YN5HCIiOgY5Pf3Bfv27cPWrVtLv2/btg0bN27EiBEj0NTUhBtuuAHf/va3ceqpp2L8+PH45je/ibFjx+JTn/oUAOADH/gALrzwQnz5y1/GT3/6U4RhiHnz5uGzn/0sxo4dO2BfjIiIjlH9LUN87LHHDlrSOHv2bBGJS+K/+c1vSkNDg6TTaZk+fbps2bKl7D12794tV111ldTU1Egmk5FrrrlG9u7d+57HwHJ4NjY2tmOjHUo5vBIRgWN6enpQW1s71MMgIqLDlMvl+l234ERVIRERUR8GFxEROYXBRURETmFwERGRUxhcRETkFAYXERE5hcFFREROYXAREZFTGFxEROQUBhcRETmFwUVERE5hcBERkVMYXERE5BQGFxEROYXBRURETmFwERGRUxhcRETkFAYXERE5hcFFREROYXAREZFTGFxEROQUBhcRETmFwUVERE5hcBERkVMYXERE5BQGFxEROYXBRURETmFwERGRUxhcRETkFAYXERE5hcFFREROYXAREZFTGFxEROQUBhcRETmFwUVERE5hcBERkVMYXERE5BQGFxEROYXBRURETmFwERGRUxhcRETkFAYXERE5hcFFREROYXAREZFTGFxEROQUBhcRETmFwUVERE5hcBERkVMYXERE5BQGFxEROYXBRURETmFwERGRUxhcRETkFAYXERE5hcFFREROYXAREZFTGFxEROQUBhcRETmFwUVERE5hcBERkVMYXERE5BQGFxEROaVfwbV48WKcffbZGDZsGEaPHo1PfepT2LJlS1mffD6PuXPnor6+HjU1Nbj88svR2dlZ1qe9vR0tLS2oqqrC6NGjMX/+fERRdPjfhoiIjnn9Cq41a9Zg7ty5+MMf/oDW1laEYYgLLrgA+/fvL/W58cYb8bvf/Q7Lli3DmjVr8Prrr+Oyyy4r7TfGoKWlBcViEc888wx+8YtfYOnSpVi4cOHAfSsiIjp2yWHo6uoSALJmzRoREenu7pYgCGTZsmWlPi+//LIAkLa2NhERefjhh0VrLR0dHaU+S5YskUwmI4VC4aCfk8/nJZfLldr27dsFABsbGxub4y2Xy/U7ew7rGlculwMAjBgxAgCwYcMGhGGI5ubmUp8JEyagqakJbW1tAIC2tjZMnjwZDQ0NpT4zZsxAT08PNm/efNDPWbx4MWpra0tt3LhxhzNsIiJy2CEHl7UWN9xwAz7ykY9g0qRJAICOjg6kUinU1dWV9W1oaEBHR0epz1tDq29/376DWbBgAXK5XKlt3779UIdNRESO8w/1hXPnzsWLL76Ip556aiDHc1DpdBrpdPqIfw4RER39DmnGNW/ePDz44IN47LHHcMIJJ5S2NzY2olgsoru7u6x/Z2cnGhsbS33+tsqw7/e+PkRERO+kX8ElIpg3bx6WL1+O1atXY/z48WX7p06diiAIsGrVqtK2LVu2oL29HdlsFgCQzWaxadMmdHV1lfq0trYik8lg4sSJh/NdiIjoeNCfSo5rr71Wamtr5fHHH5cdO3aU2oEDB0p95syZI01NTbJ69WpZv369ZLNZyWazpf1RFMmkSZPkggsukI0bN8rKlStl1KhRsmDBgvc8jlwuN+SVMGxsbGxsh98OpaqwX8H1Th/885//vNSnt7dXrrvuOhk+fLhUVVXJpZdeKjt27Ch7n1dffVVmzpwplZWVMnLkSLn55pslDMP3PA4GFxsbG9ux0Q4luFQSSE7p6elBbW3tUA+DiIgOUy6XQyaT6ddruFYhERE5hcFFREROYXAREZFTGFxEROQUBhcRETmFwUVERE5hcBERkVMYXERE5BQGFxEROYXBRURETmFwERGRUxhcRETkFAYXERE5hcFFREROYXAREZFTGFxEROQUBhcRETmFwUVERE5hcBERkVMYXERE5BQGFxEROYXBRURETmFwERGRUxhcRETkFAYXERE5hcFFREROYXAREZFTGFxEROQUBhcRETmFwUVERE5hcBERkVMYXERE5BQGFxEROYXBRURETmFwERGRUxhcRETkFAYXERE5hcFFREROYXAREZFTGFxEROQUBhcRETmFwUVERE5hcBERkVMYXERE5BQGFxEROYXBRURETmFwERGRUxhcRETkFAYXERE5hcFFREROYXAREZFTGFxEROQUBhcRETmFwUVERE5hcBERkVMYXERE5BQGFxEROYXBRURETmFwERGRUxhcRETkFAYXERE5pV/BtWTJEkyZMgWZTAaZTAbZbBYrVqwo7c/n85g7dy7q6+tRU1ODyy+/HJ2dnWXv0d7ejpaWFlRVVWH06NGYP38+oigamG9DRETHvH4F1wknnIDbb78dGzZswPr163H++efjk5/8JDZv3gwAuPHGG/G73/0Oy5Ytw5o1a/D666/jsssuK73eGIOWlhYUi0U888wz+MUvfoGlS5di4cKFA/utiIjo2CWHafjw4XLXXXdJd3e3BEEgy5YtK+17+eWXBYC0tbWJiMjDDz8sWmvp6Ogo9VmyZIlkMhkpFArv+TNzuZwAYGNjY2NzvOVyuX7nziFf4zLG4N5778X+/fuRzWaxYcMGhGGI5ubmUp8JEyagqakJbW1tAIC2tjZMnjwZDQ0NpT4zZsxAT09PadZ2MIVCAT09PWWNiIiOT/0Ork2bNqGmpgbpdBpz5szB8uXLMXHiRHR0dCCVSqGurq6sf0NDAzo6OgAAHR0dZaHVt79v3ztZvHgxamtrS23cuHH9HTYRER0j+h1cp59+OjZu3Ii1a9fi2muvxezZs/HSSy8dibGVLFiwALlcrtS2b99+RD+PiIiOXn5/X5BKpXDKKacAAKZOnYp169bhhz/8Ia688koUi0V0d3eXzbo6OzvR2NgIAGhsbMSzzz5b9n59VYd9fQ4mnU4jnU73d6hERHQMOuz7uKy1KBQKmDp1KoIgwKpVq0r7tmzZgvb2dmSzWQBANpvFpk2b0NXVVerT2tqKTCaDiRMnHu5QiIjoeNCfSo5bbrlF1qxZI9u2bZMXXnhBbrnlFlFKySOPPCIiInPmzJGmpiZZvXq1rF+/XrLZrGSz2dLroyiSSZMmyQUXXCAbN26UlStXyqhRo2TBggX9qihhVSEbGxvbsdEOpaqwX8H1xS9+UU488URJpVIyatQomT59eim0RER6e3vluuuuk+HDh0tVVZVceumlsmPHjrL3ePXVV2XmzJlSWVkpI0eOlJtvvlnCMOzXoBlcbGxsbMdGO5TgUiIicExPTw9qa2uHehhERHSYcrkcMplMv17DtQqJiMgpDC4iInIKg4uIiJzC4CIiIqcwuIiIyCkMLiIicgqDi4iInMLgIiIipzC4iIjIKQwuIiJyCoOLiIicwuAiIiKnMLiIiMgpDC4iInIKg4uIiJzC4CIiIqcwuIiIyCkMLiIicgqDi4iInMLgIiIipzC4iIjIKQwuIiJyCoOLiIicwuAiIiKnMLiIiMgpDC4iInIKg4uIiJzC4CIiIqcwuIiIyCkMLiIicgqDi4iInMLgIiIipzC4iIjIKQwuIiJyCoOLiIicwuAiIiKnMLiIiMgpDC4iKqMAaBX/m+ho5A/1AIjovUt5GqOqKwCJf38jX8SBMDrs982kA9SkfCil8NHxjWiqrcaLnW9gU8ceHAgN9hwoHPZnEA0UJSIy1IPor56eHtTW1g71MIgGRcrTOOeE0YAIqlMBJo+ug7IWogR/3rMPO3uLAACBIJcv4sWu7vf0vmNqKjB++DAorXBiXTXeN6wKvuchnU5BAYiMgYig60Aef9zZg3xo8Gx755H7onRcyuVyyGQy/XoNg4voKKIApD0NQKGhpgIfPaEB1gpG11RCrMAYAaxF4MXn8qxSCIIAWikYEyGygt0H8hBYKE9DKYXfbdmOorEAgPdlqvAPJ4yGWIuqwMewihREawgEygqUUtBQMFagNOB5Gjrw4Pkeokiwc/8BiFJYtnErjLXoDSO4918QOpowuIgcpJXC+4cPK82ozh07CkoE1lgIFCDx1SZJ/lcpFTcAEEHge4BS0IhnXQCgReCnfHi+B2gFrRWsEVhr4/e1FmFkobWCQGAEsGKhbPxBAoFWgNYK2tOApxH4PtIVAYB4W5AK8PuX/4pcbx7b39iLXC9PJ1L/HUpw8RoX0RCpTadw9piRgACnjsjEoSICE1mEkYG1AigFEUApFSeXQhwk1kIJABEUI4HWCp6Km1JAJAKBgYVGKvBgI0EYGhhjYYyFtRYKQDE0KEQGWmt4WgM2/hgrAgVAYKGVgedrGN8iDA0EglQQoJgPcf77x0IHPtr39GDb7hwe/9P2IT2mdHzgjItoEGkAtRUpnD1mNMbUVCNQQBhFUAJYG8+yrABGLJTSsMkMSikFrXQyo4pnW31NKwURgac1rAh8rSEiqAg8KK3g+x4giAPLCEITQcTCAihEcVh62oPWfTO75D8JAlhr4WkVfwYstNLwfRVXHSqFiooUvMBHkA6gPI2CNWh9+a/Y/sZe9OSLg3+AyTmccREdpYalfJw4rBqBVjizcRSMFdgwwj5jEBkDT+mk/FzBiEBEQSSOLQHgJaf0gPjfGohnYgCUEogIQmuhkn/yUQgLxLOoUErFFgrxbEpE4lBUgK88WAEiY0uzrVhyctLGpxFFBBALXYxPIyqtEUYCpQtIpXxoXyMd+Lj49Ca80VvE86914f927C5dXyMaKAwuoiPIUwpnNgzHqIo0agMPYWixb38vlNaIbF+IAKKRzJoApePTg9YCogQKKr7WpRSsxP0EKp4N9Z1KFCC0Bp7WMH0hFhkEnkpCD8gbSWLPJoEXz9ySy1rxaUq8ZcaVMCLJDZ9xtBathVKAshbGhlAK6C2E8HyNwFMIPA9aA+eOqce4mkr8dW8vnnttJ5w7tUNHLQYX0RGQSfmor0jj7IYRgAisERRCA2uB0BgoL75+BYtSsUV8Og4A4t8DH0BfuEjSX+IgMzaexZjkdysCreIqwtBYGAEiBUDFV7qgFKxSiEwcWp4XVw+K2OS0n4aIApRAJzHVN/NSfacQRQAbf44RgRagaAwgAs9TiIxBEYCv49OXUEDG93B2w3CYKMJrew9gJ+8HowHA4CIaQJ5S+OCIGoyurECV5yFfDAGJgwFawaLv2hXgI/6Pu6c9iAiU1m+GmNZJmXl8mq/vVKLAxAUaSiGyNgm7vn+AKClpFwEisRCrEGgvnrUpBVGAsQJjLDyt4wIPxKcblVZQfRWFyacB8TaVXOcysHGpfFzSCEmKOYxNTiMCCM2bpy91UaGnN4/3V6XRVBngmR17sKtw+DdM0/GNwUU0ANJa4aRMNZpqKpFWgE1mI0nROnytIKJKoaUAhMYi7XlJ6XlcYKGTAgtjDDTicnRj41mVSmZkfZeg+mZoSr05I4pPK6J0QlCsAQwgYhCJIIoiGLHxKUml4Ckd3w8GiX8WiZtWpbHrvtL7pBxEq/jf1hogOQ0ZXwOLw9LTKi65F0FkLWySbsZavL86hbyxOBDFxSFEh4LBRXQYAgVUexoT66pRnUrBBxBagbUCUYBW8Ym3yBqI1ei7yqQRz6KKxkCr+LoQkutW8alBC6sAMUmxhkVcLQgASdCZJMGMtbAQRMYi8Lw42FRcjAHVd/0quZalgL5TkSJA/EqVBF5ytUupuMJR4pmVgUWgvaRWIynssBYiFr7WMBLPuOJrb4jDylqIFRSNQWgMQmPRGxkYK3hfysc+z6KzGCHihS86BAwuokNU52tUKIUqX2PXgQJ6Q4PKwEfK8wDEoSUqDqB4mmVRNECYXJ8alkrBVwC0hzAygBIoGxdTKLxZhKGS2UokClr7cZj0XX9CfBrPEyCEoBiF8LWHUCyMtfCUTt5Hl04xasQhCPRlUVxdqJCU2ydVh/ElLUlOW8anICNj4tmgSoo5rLw5oxKJQ9TGs63QGOQjg97IILQWoUUSk/Fp0saUj9cLEWde1G8MLqJ+SmuF+sCLZxmIq/V8BRiJYERQ6dvk/iodrzwBJKf5BJEAB0KDvBHsD0PUp9MIPIuU58HDm8UXffcbS1KU4SenDGEjaM+DQbyCOwBoKCitUe0r9BbycaWi7TvlB2g/DjsrgJX4FKSxSa2Hja9naa3ilTOA0qk/ICm+t4IouYZmTJTM8OLvVEzWMxQliCIDi/hetHwUz7JCm1wxUwqeB3gSzzm1UthZDBladEgYXET9VLCC3UWDWt9LTusBWsfrC/aGFtYKAq0RaEBLfLuwlxRdeAAqPQWVzEj2Fgqo8n14QRxQno6r+5TW6LvKFEUGSilU+T4iK0gpjaI1CLROyuQ1IsRh4isNKxaBfrOEXiEu0lAAvGTGo+ICwmT1DZtcN1OlgFXJfVsKgKi4itEYA5vMFpHcDC02ntnFhSIa+TCCFQWx8QyurjKNKJkdFqI4/tK+Qq8xKBR4gzIdGgYX0SEoiKDHGJyQTgFQSPkeAu3BmvheKmsNilbie6jEwlcKgafhexopaGgFRLAIBEAYoRAZQIDA0/B0MptTgBgbXxPTGgXtwfM0bBRX5RnPg6810imF0PbNzDyIBTzPQ5isbhGXx1v4SgMSX8vykmtZklybAuKbnFVyr5gShTA0ccBpDSsGEtn4mpkIrIqvc4XWotcY7C1GGOZ78LSHQPuo9DW01kilAtjktGU6XvIQw6rS+P/296KXNybTIWJwER2iSIDKtB+HDxQqAh/i6eTG4Xj2ZEx8S6+nFHryIao8D8rz0GuByAiqJF6V3dca1lqYYrz8k1ZA3lqkk2tRaa3RbYsQBVR7HqpSPnwdF37kezV8peMy92Sm5KsIvqdRFQTIiyRhlFQrKgVPJ6cik5lgIYygNFAICygYA9h4TcNQ4tU4esUiEkHBClJKIRLBAWORtwa+Uhjm+/CSZT6UxCuBpH0PvqdhRSESgyClkfI87C4U8VzXG0P250buY3ARHaJIBFv39uK0YZWoVIAPgfI8eMlsQwQQY+ICdQEqtB9X7EEhtCHS2oMVi11hiF4RVCqFCIJ9xsJTQK8VVGkNpYAgKapQUPBMhJOVQpcpolJrGACV2kOl1tgfRajUCjlrUed7KBaKsEmVYd+6h57S8MMIklz3Co1FaCx2mQgHogj7rUEhKdAoJtfcDohFldYIRTDK9+ErhUqtUO+n4L1lFXmlNQLfg59UU4qNl5lKeRoV6QAVgY8n//La0P2h0TGBwUV0GHqNxUu5A/jQiJp4YSaJr/X0LUqbSvnwdfw8K600rIlXaB+ZTkNLPKtpSBv0RCGGaQ0lCkYseqxB3gpeD0MESiFnIlgBfKVQMBavF+PrQ33LNVUpjVrPwwFrsc8aVCeBVp3c65XxPKST+73i05wWRbHIi5RW34gkvqE5SErtfaUwTHsQAJMqqlDVd8+Z1vBVfE9aBIHveShEIXQqBaU1KtNpGGNQDENoo5BO+UilfChf47mdb6C7EA7RnxYdKxhcRIcpFMGm7v2YMnwYqhUQmRApFZecB1rD8zQCz4fv+/HNu1YQhREqPA1jLSqLEWqiEMrGK09YazEccSXh5NIyUMABa1Hj+dhRLGBHsYhKrdCbBM/eKEKN9qBMhHodILQWKaVxwMSPIdlvLTzfh5H4xuRa38NeA4wLAhRFUKE1IhFUa42Rvg8LQY3nQUPDQBBoD9rzoD0fnuchgo2LNhCfbqxIrukpzwOshY0iRCZCOhXAKoHnazzX9QZe2tU9dH9QdMxgcBENgLyx2PTGXnx4eAYVXjzbslAoRhF8a+CpuMRcJzfypjwN3/cRKI3ICxEV42dsWWvhI378iZ+sTaiT52wNUwoCjfGpFE4C4vuyAACCYqnIQpDSGpGx8ATIRxGMjUvWM6k08iaE7/lQSiFfKtjoq2iM1zz0oErvJYjvz/KUju830wrK9zFqWA0OhEWYuGYRJoqgi0UUrEFkDQpRFC/6Kx7Sng8o4E97ckP0p0PHGgYX0QDpNRbr9uQwdUQtqr34KcRGBLAqfjCkshBP4rBIysm1EvieD53WsFbgQZDyvNINwH7ygEixFl5phYu42EKrvipAiyqlktN3cQ4ZayFRhGoTF1fETzPWqAFgkpL4muQaVxxQ8WlEi7hEXmy8eK5VyV7lQaV8VAUBQitQvo+UUjDGoDcsJov+xiuChNZClACICzRECVpf3YHIcpkMGhgMLqIBFFrBC2/0YGQ6hZNrKpFW8TWi0EYo2njtvsALEGgLbfpWFozX//P8+LRiyguS9QeThZisLd1X5UFDrMSVe4jDqO8G38DzYGz85GQFQKfSsNZAJQFoBTA2rvizQOm+L0HyaJW++LIC0RqAAJ5GaAy058H6HlJBCp4VhMUiImuRD0OENkIUxc8VKyY3KAMKqSCeEj6zYxc69+eH6E+EjkX6cF58++23QymFG264obQtn89j7ty5qK+vR01NDS6//HJ0dnaWva69vR0tLS2oqqrC6NGjMX/+fEQRV4ymY8MBY9F+II9N3ftgEK/5Z5MHN/at/Ve0EXrDInoLBRSKRUQmQiEqwoogEgvl+fBTaQRBXPAQBClUVlQjXVmFdFUVKiurEVRUQvk+PN9D4PsQCDwv/jkVpOCn0whSlfBTafgVlUhXVyOorEJQkUYqFSDlB/A9D4HnxwUX2oOfXMsKfB86COD5PlLpCnipFLTnQ7SGUUC+GCKKonhppyhCwYQwYuKZHgS+p1BdEaAAix37GFo0sA55xrVu3Tr853/+J6ZMmVK2/cYbb8RDDz2EZcuWoba2FvPmzcNll12Gp59+GgBgjEFLSwsaGxvxzDPPYMeOHfj85z+PIAjwne985/C+DdFRZHcxxMbuHkyuG4aKZM2K0Bp4SiUPkLTIW0FaezA6vpk4zBfgpYGiBXzPQnvxtaWKisr41J3tW/kiqWD0Aqh4+QsEWscL8RoLo1RcKBFZ9Pb2Ip2ugPY8wETxVSkTATa5mdhKXEmYLMrbtxq952mIjisHLeIyekBgjYGGoJhUOurkWWImKbv3PYWqigD7RfDYX7uG9M+Ajk1K+m6b74d9+/bhzDPPxE9+8hN8+9vfxoc+9CH84Ac/QC6Xw6hRo/DLX/4SV1xxBQDgj3/8Iz7wgQ+gra0N5557LlasWIGLL74Yr7/+OhoaGgAAP/3pT/Ev//Iv2LlzJ1Kp1N/9/J6eHtTW1vZ32ERDYlRFCtnR9SgWw7jowouXiupblzCVzHaSh4agKkjBT6r3vGTGVRkEEFHwktfqpIBCJwvnGolX7IDEaw9apeIVOMIIYbGI6soKeIGPKFnFQsRCG4MoKl8w19MaheRxLCnfh4GKl5MSwNgIhSieaYVRiNDEj0ox1iAUg3wUwvcU/MDDPmvx9Ou7sbfI0nd6d7lcDplMpl+vOaRThXPnzkVLSwuam5vLtm/YsAFhGJZtnzBhApqamtDW1gYAaGtrw+TJk0uhBQAzZsxAT08PNm/efNDPKxQK6OnpKWtErtiZL2Ltzj3IK4HoeN0/2/cIELHIG4OijZ+TFUmE/VEReROiEBURRUX05nuxv7cXYVhEbz6PA/lemKiIMCrGpfLagygf0D5E+xDtwSodF1YohUgp9EYGeRMhAqA8H0b5EC+A8gNESkN7AQwUDDQ8PwU/SCOUeBYlolAMiziQz6NQLKC3mI+vZYlF0YaIrIEVg3SgYbVCLoywqr2ToUVHTL9PFd5777147rnnsG7durft6+joQCqVQl1dXdn2hoYGdHR0lPq8NbT69vftO5jFixfjtttu6+9QiY4anb0FdPYWcHptDSaPyMAaiZ+XlRRFRNZCeyqZSVkUTQgjXnyDcfzUkXiGpjQ8BRSKBlDxQrxRUgxh5c3HmBiVPMQxiles742K8BGvg1idrkDRWhjE5R/FyCJIB1DKILSm9OiUyMSBFNkIJgpRLBaScI2vY5lkZXmj4gpEozTWdu5BZ29hSI81Hfv6FVzbt2/H9ddfj9bWVlRUVBypMb3NggULcNNNN5V+7+npwbhx4wbt84kGyiu5fUj7HibWZ5C2PsJiVHpulk5WZ1fJQrYmuXlYKw2xGtDxU4e95GbhvseeeNaPF8KNgJTvwVoDUXEloylGyTJTQGgVUp6GFkBrH/mwGK8MD0ExilCMwuT0o0UEIDIRiqaIyBrkw0L8RGMIisbGi+8qBYs4cMVqPNGxG7vyXPGdjrx+BdeGDRvQ1dWFM888s7TNGIMnnngCd955J37/+9+jWCyiu7u7bNbV2dmJxsZGAEBjYyOeffbZsvftqzrs6/O30uk00ul0f4ZKdFSyAF7YncOLe3JobmpEQ0UaxliYyCQ3/yJZEDe5YRmA8uInIxejEBAgSpZu6nsoJGARFgy0FyAKAc9TgAKKJg4j3w+SRXwNQrHQAoRRL2yyfJOxBmFUhDW2tFp8PAs0yJsQxkaIbDw/k2SFdx1fkIOnNUKt8NRrOxlaNGj6dY1r+vTp2LRpEzZu3FhqZ511FmbNmlX6OQgCrFq1qvSaLVu2oL29HdlsFgCQzWaxadMmdHW9WW3U2tqKTCaDiRMnDtDXIjq6WQGefG0ndkZFVFYGSFcE8HwN5cUPdfQ8BV/rZIHeOLSKUVQqqe+7+TiyBoXkWpKxEYomRNHERRQQiSsDk9U3oAT5KMT+Yh5h3wMhxZRC0VgDwMKIQWhCRDaK9yePQIFW0Coumfd9jXSFj809+/DUjl3oOMCSdxo8/ZpxDRs2DJMmTSrbVl1djfr6+tL2L33pS7jpppswYsQIZDIZfPWrX0U2m8W5554LALjgggswceJEfO5zn8N3v/tddHR04P/9v/+HuXPnclZFx5XeyGDVq52oTvm4+P1jUZ1OIypGiML4ZuOU55UeVaKgoLUHrZMHPSYrZMQlwfH1LJO8ryBend6KIOX5yEcRCmERoYniR60AqPA1ImsR9l1nS4qLIysITTzDMmKSVTbiGZxSGumUB89T2Gcj/N/uHF7Y2c2nGNOgG/CVM/7jP/4DWmtcfvnlKBQKmDFjBn7yk5+U9nuehwcffBDXXnststksqqurMXv2bHzrW98a6KEQHfVCa9GdL2Llth34wMgMTh9eDd/XsJHET08WDZMs2xSJhYYGrMAgnnVJch9V8qxieJ4HQFAIQ3jaQ17CpAgkfqilMQbQGkUTxacaJS7CMNZAANhkrUERQWgtdPJwycDTSAUe3ohC7Njfi2de2wXb/ztpiAbEId3HNdR4Hxcdq8YOq4AHhWlj6zEiFUBZQOK6doRR/NgRsfGagKXHp0hcKJE8zxJK6VK5ffxg5PhBln3rHPrJc8BE4veJTF9YAYL4Pi+VLJ7r+xp+oKE9hcdf24WdvQW8UeC1LBo4h3IfF9cqJDqKvL43vlb0+p/ihy1OGVWLk+uqkfYDjMikUewNEYYWobUIlA+drFNorI3DSwRaxzcUK6WgVbzclNLxE5K1iisTjTEQsbAACiaK+2oNEYXA9yBiUZHysatQxOu5PNZ1vBEvGEx0FOCMi8gBI6vSGD98GKqVxgeHZxCGEZTE92FFpu8+r/gpx1p5ySK6FoHnxQUdybocJjIItAdro/hUX1J673k6eYoxoCDYb0JsemMvtryxD/tD8+6DIzoMnHERHaN2HShg14ECPKXwwq7u0mNPFIDm941CpeeVrjkpZQGlUZHy4WkPfnJK0RoLXwP5METBRNA6PsUILTAQPPbaTuwN48WujQh6ilz4mo5OnHERHYPSnsb08WMABdi+x6MYgTURNu/K4fX9XN2Cjg6ccRERAKBgLB7e+tpQD4PoiDis53ERERENNgYXERE5hcFFREROYXAREZFTGFxEROQUBhcRETmFwUVERE5hcBERkVMYXERE5BQGFxEROYXBRURETmFwERGRUxhcRETkFAYXERE5hcFFREROYXAREZFTGFxEROQUBhcRETmFwUVERE5hcBERkVMYXERE5BQGFxEROYXBRURETmFwERGRUxhcRETkFAYXERE5hcFFREROYXAREZFTGFxEROQUBhcRETmFwUVERE5hcBERkVMYXERE5BQGFxEROYXBRURETmFwERGRUxhcRETkFAYXERE5hcFFREROYXAREZFTGFxEROQUBhcRETmFwUVERE5hcBERkVMYXERE5BQGFxEROYXBRURETmFwERGRUxhcRETkFAYXERE5hcFFREROYXAREZFTGFxEROQUBhcRETmFwUVERE5hcBERkVP6FVy33norlFJlbcKECaX9+Xwec+fORX19PWpqanD55Zejs7Oz7D3a29vR0tKCqqoqjB49GvPnz0cURQPzbYiI6Jjn9/cFH/zgB/Hoo4+++Qb+m29x44034qGHHsKyZctQW1uLefPm4bLLLsPTTz8NADDGoKWlBY2NjXjmmWewY8cOfP7zn0cQBPjOd74zAF+HiIiOedIPixYtkjPOOOOg+7q7uyUIAlm2bFlp28svvywApK2tTUREHn74YdFaS0dHR6nPkiVLJJPJSKFQeM/jyOVyAoCNjY2NzfGWy+X6E0MiItLva1yvvPIKxo4di5NPPhmzZs1Ce3s7AGDDhg0IwxDNzc2lvhMmTEBTUxPa2toAAG1tbZg8eTIaGhpKfWbMmIGenh5s3rz5HT+zUCigp6enrBER0fGpX8E1bdo0LF26FCtXrsSSJUuwbds2fOxjH8PevXvR0dGBVCqFurq6stc0NDSgo6MDANDR0VEWWn37+/a9k8WLF6O2trbUxo0b159hExHRMaRf17hmzpxZ+nnKlCmYNm0aTjzxRNx3332orKwc8MH1WbBgAW666abS7z09PQwvIqLj1GGVw9fV1eG0007D1q1b0djYiGKxiO7u7rI+nZ2daGxsBAA0Nja+rcqw7/e+PgeTTqeRyWTKGhERHZ8OK7j27duHP//5zxgzZgymTp2KIAiwatWq0v4tW7agvb0d2WwWAJDNZrFp0yZ0dXWV+rS2tiKTyWDixImHMxQiIjpe9KeS4+abb5bHH39ctm3bJk8//bQ0NzfLyJEjpaurS0RE5syZI01NTbJ69WpZv369ZLNZyWazpddHUSSTJk2SCy64QDZu3CgrV66UUaNGyYIFC/pVUcKqQjY2NrZjox1KVWG/guvKK6+UMWPGSCqVkve9731y5ZVXytatW0v7e3t75brrrpPhw4dLVVWVXHrppbJjx46y93j11Vdl5syZUllZKSNHjpSbb75ZwjDs16AZXGxsbGzHRjuU4FIiInBMT08Pamtrh3oYRER0mHK5XL/rFpxcq9DBrCUiooM4lP+eOxlcu3fvHuohEBHRANi7d2+/X9PvtQqPBiNGjAAQL9jLU4YH13ev2/bt23n7wEHw+Lw7Hp93x+Pz7t7L8RER7N27F2PHju33+zsZXFrHE8Xa2lr+pfk7eN/bu+PxeXc8Pu+Ox+fd/b3jc6gTDydPFRIR0fGLwUVERE5xMrjS6TQWLVqEdDo91EM5avEYvTsen3fH4/PueHze3ZE+Pk7ex0VERMcvJ2dcRER0/GJwERGRUxhcRETkFAYXERE5hcFFREROcTK4fvzjH+Okk05CRUUFpk2bhmeffXaohzQonnjiCVxyySUYO3YslFL4zW9+U7ZfRLBw4UKMGTMGlZWVaG5uxiuvvFLWZ8+ePZg1axYymQzq6urwpS99Cfv27RvEb3HkLF68GGeffTaGDRuG0aNH41Of+hS2bNlS1iefz2Pu3Lmor69HTU0NLr/88rc9lbu9vR0tLS2oqqrC6NGjMX/+fERRNJhf5YhYsmQJpkyZUlrNIJvNYsWKFaX9x/OxOZjbb78dSinccMMNpW3H8zG69dZboZQqaxMmTCjtH9Rj0+8HoQyxe++9V1KplPzsZz+TzZs3y5e//GWpq6uTzs7OoR7aEffwww/LN77xDXnggQcEgCxfvrxs/+233y61tbXym9/8Rv7v//5P/vmf/1nGjx8vvb29pT4XXnihnHHGGfKHP/xBnnzySTnllFPkqquuGuRvcmTMmDFDfv7zn8uLL74oGzdulIsuukiamppk3759pT5z5syRcePGyapVq2T9+vVy7rnnyj/8wz+U9vc97LS5uVmef/55efjhh2XkyJH9ftjp0ei3v/2tPPTQQ/KnP/1JtmzZIl//+tclCAJ58cUXReT4PjZ/69lnn5WTTjpJpkyZItdff31p+/F8jBYtWiQf/OAHZceOHaW2c+fO0v7BPDbOBdc555wjc+fOLf1ujJGxY8fK4sWLh3BUg+9vg8taK42NjfK9732vtK27u1vS6bTcc889IiLy0ksvCQBZt25dqc+KFStEKSWvvfbaoI19sHR1dQkAWbNmjYjExyMIAlm2bFmpz8svvywApK2tTUTi/3OgtZaOjo5SnyVLlkgmk5FCoTC4X2AQDB8+XO666y4em7fYu3evnHrqqdLa2ir/+I//WAqu4/0YLVq0SM4444yD7hvsY+PUqcJisYgNGzagubm5tE1rjebmZrS1tQ3hyIbetm3b0NHRUXZsamtrMW3atNKxaWtrQ11dHc4666xSn+bmZmitsXbt2kEf85GWy+UAvPk0gQ0bNiAMw7JjNGHCBDQ1NZUdo8mTJ6OhoaHUZ8aMGejp6cHmzZsHcfRHljEG9957L/bv349sNstj8xZz585FS0tL2bEA+PcHAF555RWMHTsWJ598MmbNmoX29nYAg39snFodfteuXTDGlH1xAGhoaMAf//jHIRrV0aGjowMADnps+vZ1dHRg9OjRZft938eIESNKfY4V1lrccMMN+MhHPoJJkyYBiL9/KpVCXV1dWd+/PUYHO4Z9+1y3adMmZLNZ5PN51NTUYPny5Zg4cSI2btx43B8bALj33nvx3HPPYd26dW/bd7z//Zk2bRqWLl2K008/HTt27MBtt92Gj33sY3jxxRcH/dg4FVxE79XcuXPx4osv4qmnnhrqoRxVTj/9dGzcuBG5XA73338/Zs+ejTVr1gz1sI4K27dvx/XXX4/W1lZUVFQM9XCOOjNnziz9PGXKFEybNg0nnngi7rvvPlRWVg7qWJw6VThy5Eh4nve2SpXOzk40NjYO0aiODn3f/92OTWNjI7q6usr2R1GEPXv2HFPHb968eXjwwQfx2GOP4YQTTihtb2xsRLFYRHd3d1n/vz1GBzuGfftcl0qlcMopp2Dq1KlYvHgxzjjjDPzwhz/ksUF8uqurqwtnnnkmfN+H7/tYs2YN7rjjDvi+j4aGhuP+GL1VXV0dTjvtNGzdunXQ//44FVypVApTp07FqlWrStustVi1ahWy2ewQjmzojR8/Ho2NjWXHpqenB2vXri0dm2w2i+7ubmzYsKHUZ/Xq1bDWYtq0aYM+5oEmIpg3bx6WL1+O1atXY/z48WX7p06diiAIyo7Rli1b0N7eXnaMNm3aVBbwra2tyGQymDhx4uB8kUFkrUWhUOCxATB9+nRs2rQJGzduLLWzzjoLs2bNKv18vB+jt9q3bx/+/Oc/Y8yYMYP/96ffpSVD7N5775V0Oi1Lly6Vl156Sb7yla9IXV1dWaXKsWrv3r3y/PPPy/PPPy8A5N///d/l+eefl7/+9a8iEpfD19XVyf/+7//KCy+8IJ/85CcPWg7/4Q9/WNauXStPPfWUnHrqqcdMOfy1114rtbW18vjjj5eV7B44cKDUZ86cOdLU1CSrV6+W9evXSzablWw2W9rfV7J7wQUXyMaNG2XlypUyatSoY6Kc+ZZbbpE1a9bItm3b5IUXXpBbbrlFlFLyyCOPiMjxfWzeyVurCkWO72N08803y+OPPy7btm2Tp59+Wpqbm2XkyJHS1dUlIoN7bJwLLhGRH/3oR9LU1CSpVErOOecc+cMf/jDUQxoUjz32mAB4W5s9e7aIxCXx3/zmN6WhoUHS6bRMnz5dtmzZUvYeu3fvlquuukpqamokk8nINddcI3v37h2CbzPwDnZsAMjPf/7zUp/e3l657rrrZPjw4VJVVSWXXnqp7Nixo+x9Xn31VZk5c6ZUVlbKyJEj5eabb5YwDAf52wy8L37xi3LiiSdKKpWSUaNGyfTp00uhJXJ8H5t38rfBdTwfoyuvvFLGjBkjqVRK3ve+98mVV14pW7duLe0fzGPD53EREZFTnLrGRURExOAiIiKnMLiIiMgpDC4iInIKg4uIiJzC4CIiIqcwuIiIyCkMLiIicgqDi4iInMLgIiIipzC4iIjIKf8/7ZJrD2ovxaIAAAAASUVORK5CYII=",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def preprocess_image(image_path):\n",
    "    image = cv2.imread(image_path, cv2.IMREAD_UNCHANGED)\n",
    "\n",
    "    # For HDR EXR files, clip to [0, 1] and convert to 8-bit\n",
    "    if image.dtype == np.float32:  \n",
    "        image = np.clip(image, 0, 1) \n",
    "        image = (image * 255).astype(np.uint8)\n",
    "\n",
    "    image_resized = cv2.resize(image, (512, 512))\n",
    "    image_rgb = cv2.cvtColor(image_resized, cv2.COLOR_BGR2RGB)\n",
    "    return image_rgb\n",
    "\n",
    "def lip_segmentation(image_rgb):\n",
    "    # mp_face_mesh = mp.solutions.face_mesh\n",
    "    face_mesh = mp.solutions.face_mesh.FaceMesh(\n",
    "        static_image_mode=True, max_num_faces=1, refine_landmarks=True)\n",
    "    results = face_mesh.process(image_rgb)\n",
    "    \n",
    "    lip_indices = [\n",
    "        61, 146, 91, 181, 84, 17, 314, 405, 321, 375,\n",
    "        291, 61, 185, 40, 39, 37, 0, 267, 269, 270,\n",
    "        409, 291, 78, 95, 88, 178, 87, 14, 317, 402,\n",
    "        318, 324, 308, 78, 191, 80, 81, 82, 13, 312,\n",
    "        311, 310, 415, 308\n",
    "    ]\n",
    "    \n",
    "    if results.multi_face_landmarks:\n",
    "        face_landmarks = results.multi_face_landmarks[0]\n",
    "        lip_landmarks = [face_landmarks.landmark[i] for i in lip_indices]\n",
    "        mask = np.zeros(image_rgb.shape[:2], dtype=np.uint8)\n",
    "        points = [\n",
    "            (int(landmark.x * image_rgb.shape[1]), int(landmark.y * image_rgb.shape[0]))\n",
    "            for landmark in lip_landmarks\n",
    "        ]\n",
    "        points = np.array(points, dtype=np.int32)\n",
    "        cv2.fillPoly(mask, [points], 255)\n",
    "        lip_mask = cv2.bitwise_and(image_rgb, image_rgb, mask=mask)\n",
    "        return lip_mask, mask\n",
    "    return None, None\n",
    "\n",
    "def display_image(image, title=\"Title\"):\n",
    "    plt.imshow(image)\n",
    "    plt.title(title)\n",
    "    plt.show()\n",
    "\n",
    "files = load_celeba_data(\"test_publishable_list\")\n",
    "file0 = files[0]\n",
    "pp_img = preprocess_image(os.path.join(file0, '1_Albedo.exr'))\n",
    "lip_mask, mask = lip_segmentation(pp_img)\n",
    "display_image(lip_mask, \"Lip Mask\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Lighting Estimation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "def evaluate_sh_basis_gpu(theta, phi):\n",
    "    \"\"\"Vectorized SH basis computation on GPU\"\"\"\n",
    "    sh_basis = torch.zeros((theta.shape[0], 9), device=theta.device)\n",
    "    \n",
    "    # Y00\n",
    "    sh_basis[:, 0] = 0.282095 * torch.ones_like(theta)\n",
    "    # Y1m\n",
    "    sh_basis[:, 1] = 0.488603 * torch.sin(theta) * torch.cos(phi)\n",
    "    sh_basis[:, 2] = 0.488603 * torch.sin(theta) * torch.sin(phi)\n",
    "    sh_basis[:, 3] = 0.488603 * torch.cos(theta)\n",
    "    # Y2m\n",
    "    sh_basis[:, 4] = 1.092548 * torch.sin(theta)**2 * torch.cos(2*phi)\n",
    "    sh_basis[:, 5] = 1.092548 * torch.sin(theta) * torch.cos(theta) * torch.cos(phi)\n",
    "    sh_basis[:, 6] = 0.315392 * (3*torch.cos(theta)**2 - 1)\n",
    "    sh_basis[:, 7] = 1.092548 * torch.sin(theta) * torch.cos(theta) * torch.sin(phi)\n",
    "    sh_basis[:, 8] = 0.546274 * torch.sin(theta)**2 * torch.cos(2*phi)\n",
    "    \n",
    "    return sh_basis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "def estimate_lighting_gpu(image_rgb, face_landmarks, device='cuda'):\n",
    "    \"\"\"GPU-accelerated lighting estimation using spherical harmonics\"\"\"\n",
    "    # Convert inputs to torch tensors with explicit dtype\n",
    "    image_tensor = torch.from_numpy(image_rgb).float().to(device)\n",
    "    \n",
    "    # Extract face geometry\n",
    "    image_h, image_w = image_rgb.shape[:2]\n",
    "    vertices = torch.tensor([\n",
    "        [landmark.x * image_w, landmark.y * image_h, landmark.z]\n",
    "        for landmark in face_landmarks.landmark\n",
    "    ], dtype=torch.float32, device=device)\n",
    "    \n",
    "    # Compute face normals using neighboring vertices\n",
    "    v1 = vertices[1:-1] - vertices[:-2]  # Size: N-2\n",
    "    v2 = vertices[2:] - vertices[1:-1]   # Size: N-2\n",
    "    \n",
    "    # Debug size check\n",
    "    assert v1.shape == v2.shape, f\"Shape mismatch: v1 {v1.shape} vs v2 {v2.shape}\"\n",
    "    \n",
    "    normals = torch.cross(v1, v2)\n",
    "    normals = normals / (torch.norm(normals, dim=1, keepdim=True) + 1e-6)  # Add epsilon to avoid div by 0\n",
    "    \n",
    "    # Create face mask\n",
    "    face_mask = torch.zeros((image_h, image_w), dtype=torch.float32, device=device)\n",
    "    points = vertices[:, :2].cpu().numpy().astype(np.int32)\n",
    "    face_mask = torch.from_numpy(\n",
    "        cv2.fillConvexPoly(face_mask.cpu().numpy(), points, 1)\n",
    "    ).to(device)\n",
    "    \n",
    "    # Sample face colors and positions\n",
    "    y_coords, x_coords = torch.where(face_mask > 0)\n",
    "    colors = image_tensor[y_coords, x_coords].float()  # Ensure float type\n",
    "    \n",
    "    # Convert to spherical coordinates\n",
    "    positions = torch.stack([\n",
    "        x_coords.float() / image_w * 2 - 1,\n",
    "        y_coords.float() / image_h * 2 - 1\n",
    "    ], dim=1)\n",
    "    \n",
    "    # Compute spherical harmonics basis\n",
    "    theta = torch.arccos(torch.clamp(positions[:, 1], -1, 1))\n",
    "    phi = torch.arctan2(positions[:, 0], torch.ones_like(positions[:, 0]))\n",
    "    \n",
    "    # Evaluate SH basis functions\n",
    "    sh_basis = evaluate_sh_basis_gpu(theta, phi)\n",
    "    \n",
    "    # Debug dtype check\n",
    "    assert sh_basis.dtype == colors.dtype, f\"Dtype mismatch: sh_basis {sh_basis.dtype} vs colors {colors.dtype}\"\n",
    "    \n",
    "    # Solve for coefficients using least squares\n",
    "    sh_coefficients = torch.linalg.lstsq(sh_basis, colors).solution[:9]\n",
    "    \n",
    "    return sh_coefficients.cpu().numpy()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Physically-Inspired Lipstick Rendering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Helper Functions'''\n",
    "\n",
    "def feather_mask(binary_mask, blur_radius=15):\n",
    "    # Apply Gaussian blur to create a soft gradient around given mask\n",
    "    feathered_mask = cv2.GaussianBlur(binary_mask, (blur_radius, blur_radius), 0)\n",
    "    feathered_mask = np.clip(feathered_mask, 0, 255).astype(np.uint8)\n",
    "    return feathered_mask\n",
    "\n",
    "\n",
    "def blend_albedo(albedo, lip_binary_mask, lipstick_color, blend_factor=0.5):\n",
    "    # Blend albedo with provided lipstick color\n",
    "    lipstick_layer = np.zeros_like(albedo)\n",
    "    lipstick_layer[:, :] = lipstick_color   \n",
    "    masked_lipstick = cv2.bitwise_and(lipstick_layer, lipstick_layer, mask=lip_binary_mask)\n",
    "    masked_albedo = cv2.bitwise_and(albedo, albedo, mask=lip_binary_mask)\n",
    "    blended_albedo = cv2.addWeighted(masked_albedo, 1 - blend_factor, masked_lipstick, blend_factor, 0)\n",
    "    return blended_albedo\n",
    "\n",
    "\n",
    "def adjust_specular_with_roughness(specular_map, roughness_map, intensity=1.0):\n",
    "    # Use roughness to adjust specular highlights\n",
    "    gloss_factor = 1 - roughness_map / 255.0\n",
    "    highlights = cv2.multiply(specular_map.astype(np.float64), gloss_factor)\n",
    "    highlights = (highlights * intensity).astype(np.uint8)\n",
    "    return highlights\n",
    "\n",
    "def apply_fresnel_effect(specular_map, depth_map, lip_binary_mask, intensity=1.0):\n",
    "    # Normalize depth map and compute Fresnel term\n",
    "    normalized_depth = cv2.normalize(depth_map, None, 0, 1, cv2.NORM_MINMAX)\n",
    "    fresnel_effect = 1 - np.power(normalized_depth, 2)\n",
    "    enhanced_specular = (specular_map.astype(np.float32) * fresnel_effect * intensity)\n",
    "    masked_specular = cv2.bitwise_and((enhanced_specular).astype(np.uint8),\n",
    "                                      (enhanced_specular).astype(np.uint8),\n",
    "                                      mask=lip_binary_mask)\n",
    "    return masked_specular\n",
    "\n",
    "def composite_final_image(albedo_image, albedo, diffuse, specular):\n",
    "    # Combine diffuse and specular components (adjustable)\n",
    "    blended_mask = cv2.addWeighted(diffuse, 0.1, specular, 0.8, 0)\n",
    "    # Combine lighting components with lipstick color (adjustable)\n",
    "    blended_mask = cv2.addWeighted(albedo, 1.0, blended_mask, 0.8, 0)\n",
    "    # Apply combined masks in a smooth gradient manner\n",
    "    feathered_mask = feather_mask(blended_mask, blur_radius=7)\n",
    "    blended_mask = cv2.addWeighted(albedo_image, 1.0, feathered_mask, 1.0, 0)\n",
    "    return blended_mask\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Main Processing Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "path  test_publishable_list exists  True\n",
      "Processed and saved outputs for test_publishable_list_2_340\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0000 00:00:1733642614.908597  280999 gl_context.cc:357] GL version: 2.1 (2.1 Metal - 88.1), renderer: Apple M1\n",
      "W0000 00:00:1733642614.909823  295002 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1733642614.914116  295002 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "I0000 00:00:1733642615.044997  280999 gl_context.cc:357] GL version: 2.1 (2.1 Metal - 88.1), renderer: Apple M1\n",
      "W0000 00:00:1733642615.046410  295021 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1733642615.051075  295021 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed and saved outputs for test_publishable_list_2_527\n",
      "Processed and saved outputs for test_publishable_list_3_527\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0000 00:00:1733642615.195804  280999 gl_context.cc:357] GL version: 2.1 (2.1 Metal - 88.1), renderer: Apple M1\n",
      "W0000 00:00:1733642615.196881  295027 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1733642615.200659  295027 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "I0000 00:00:1733642615.354033  280999 gl_context.cc:357] GL version: 2.1 (2.1 Metal - 88.1), renderer: Apple M1\n",
      "W0000 00:00:1733642615.355470  295046 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1733642615.360177  295046 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed and saved outputs for test_publishable_list_3_340\n",
      "Processed and saved outputs for test_publishable_list_1_340\n",
      "Missing image path!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Context leak detected, msgtracer returned -1\n",
      "I0000 00:00:1733642615.497010  280999 gl_context.cc:357] GL version: 2.1 (2.1 Metal - 88.1), renderer: Apple M1\n",
      "W0000 00:00:1733642615.498144  295072 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1733642615.502870  295069 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "I0000 00:00:1733642615.635653  280999 gl_context.cc:357] GL version: 2.1 (2.1 Metal - 88.1), renderer: Apple M1\n",
      "W0000 00:00:1733642615.637273  295076 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1733642615.641660  295076 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed and saved outputs for test_publishable_list_0_527\n",
      "Processed and saved outputs for test_publishable_list_0_340\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0000 00:00:1733642615.828230  280999 gl_context.cc:357] GL version: 2.1 (2.1 Metal - 88.1), renderer: Apple M1\n",
      "W0000 00:00:1733642615.829360  295109 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1733642615.833877  295108 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "I0000 00:00:1733642615.967442  280999 gl_context.cc:357] GL version: 2.1 (2.1 Metal - 88.1), renderer: Apple M1\n",
      "W0000 00:00:1733642615.968643  295127 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1733642615.972985  295127 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed and saved outputs for test_publishable_list_4_340\n",
      "Processed and saved outputs for test_publishable_list_4_527\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0000 00:00:1733642616.106181  280999 gl_context.cc:357] GL version: 2.1 (2.1 Metal - 88.1), renderer: Apple M1\n",
      "W0000 00:00:1733642616.107832  295142 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1733642616.111917  295142 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "I0000 00:00:1733642616.251612  280999 gl_context.cc:357] GL version: 2.1 (2.1 Metal - 88.1), renderer: Apple M1\n",
      "W0000 00:00:1733642616.252713  295151 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1733642616.256663  295153 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed and saved outputs for test_publishable_list_5_527\n",
      "Processed and saved outputs for test_publishable_list_5_340\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0000 00:00:1733642616.397281  280999 gl_context.cc:357] GL version: 2.1 (2.1 Metal - 88.1), renderer: Apple M1\n",
      "W0000 00:00:1733642616.398503  295166 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1733642616.402807  295164 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "I0000 00:00:1733642616.527110  280999 gl_context.cc:357] GL version: 2.1 (2.1 Metal - 88.1), renderer: Apple M1\n",
      "W0000 00:00:1733642616.528473  295171 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1733642616.533173  295172 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed and saved outputs for test_publishable_list_6_527\n",
      "Processed and saved outputs for test_publishable_list_6_340\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0000 00:00:1733642616.685374  280999 gl_context.cc:357] GL version: 2.1 (2.1 Metal - 88.1), renderer: Apple M1\n",
      "W0000 00:00:1733642616.686890  295178 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1733642616.691420  295178 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "I0000 00:00:1733642616.821448  280999 gl_context.cc:357] GL version: 2.1 (2.1 Metal - 88.1), renderer: Apple M1\n",
      "W0000 00:00:1733642616.822540  295186 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1733642616.828168  295191 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed and saved outputs for test_publishable_list_0_610\n",
      "Processed and saved outputs for test_publishable_list_6_594\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0000 00:00:1733642616.970520  280999 gl_context.cc:357] GL version: 2.1 (2.1 Metal - 88.1), renderer: Apple M1\n",
      "W0000 00:00:1733642616.971913  295197 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1733642616.977631  295194 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "I0000 00:00:1733642617.128847  280999 gl_context.cc:357] GL version: 2.1 (2.1 Metal - 88.1), renderer: Apple M1\n",
      "W0000 00:00:1733642617.130027  295207 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1733642617.134159  295209 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed and saved outputs for test_publishable_list_1_610\n",
      "Processed and saved outputs for test_publishable_list_3_610\n",
      "Missing image path!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0000 00:00:1733642617.275349  280999 gl_context.cc:357] GL version: 2.1 (2.1 Metal - 88.1), renderer: Apple M1\n",
      "W0000 00:00:1733642617.276658  295221 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1733642617.280473  295221 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "I0000 00:00:1733642617.415334  280999 gl_context.cc:357] GL version: 2.1 (2.1 Metal - 88.1), renderer: Apple M1\n",
      "W0000 00:00:1733642617.416551  295226 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1733642617.420946  295226 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed and saved outputs for test_publishable_list_5_594\n",
      "Processed and saved outputs for test_publishable_list_2_610\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0000 00:00:1733642617.576572  280999 gl_context.cc:357] GL version: 2.1 (2.1 Metal - 88.1), renderer: Apple M1\n",
      "W0000 00:00:1733642617.577793  295243 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1733642617.582276  295243 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "I0000 00:00:1733642617.713205  280999 gl_context.cc:357] GL version: 2.1 (2.1 Metal - 88.1), renderer: Apple M1\n",
      "W0000 00:00:1733642617.714480  295270 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1733642617.718835  295270 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed and saved outputs for test_publishable_list_6_610\n",
      "Processed and saved outputs for test_publishable_list_1_594\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0000 00:00:1733642617.860224  280999 gl_context.cc:357] GL version: 2.1 (2.1 Metal - 88.1), renderer: Apple M1\n",
      "W0000 00:00:1733642617.861413  295279 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1733642617.865429  295285 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "Context leak detected, msgtracer returned -1\n",
      "I0000 00:00:1733642618.010556  280999 gl_context.cc:357] GL version: 2.1 (2.1 Metal - 88.1), renderer: Apple M1\n",
      "W0000 00:00:1733642618.011778  295290 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1733642618.016045  295289 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed and saved outputs for test_publishable_list_0_594\n",
      "Processed and saved outputs for test_publishable_list_5_610\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0000 00:00:1733642618.173694  280999 gl_context.cc:357] GL version: 2.1 (2.1 Metal - 88.1), renderer: Apple M1\n",
      "W0000 00:00:1733642618.175091  295304 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1733642618.178998  295304 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "I0000 00:00:1733642618.310079  280999 gl_context.cc:357] GL version: 2.1 (2.1 Metal - 88.1), renderer: Apple M1\n",
      "W0000 00:00:1733642618.311365  295308 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1733642618.315149  295308 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed and saved outputs for test_publishable_list_2_594\n",
      "Processed and saved outputs for test_publishable_list_3_594\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0000 00:00:1733642618.466297  280999 gl_context.cc:357] GL version: 2.1 (2.1 Metal - 88.1), renderer: Apple M1\n",
      "W0000 00:00:1733642618.467486  295317 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1733642618.471691  295322 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "I0000 00:00:1733642618.616853  280999 gl_context.cc:357] GL version: 2.1 (2.1 Metal - 88.1), renderer: Apple M1\n",
      "W0000 00:00:1733642618.618125  295328 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1733642618.622611  295328 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed and saved outputs for test_publishable_list_4_610\n",
      "Processed and saved outputs for test_publishable_list_2_122\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0000 00:00:1733642618.793131  280999 gl_context.cc:357] GL version: 2.1 (2.1 Metal - 88.1), renderer: Apple M1\n",
      "W0000 00:00:1733642618.794346  295349 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1733642618.798856  295353 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "I0000 00:00:1733642618.935533  280999 gl_context.cc:357] GL version: 2.1 (2.1 Metal - 88.1), renderer: Apple M1\n",
      "W0000 00:00:1733642618.936723  295359 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1733642618.940957  295358 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed and saved outputs for test_publishable_list_3_344\n",
      "Processed and saved outputs for test_publishable_list_2_344\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0000 00:00:1733642619.077716  280999 gl_context.cc:357] GL version: 2.1 (2.1 Metal - 88.1), renderer: Apple M1\n",
      "W0000 00:00:1733642619.078911  295366 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1733642619.082833  295364 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "I0000 00:00:1733642619.217685  280999 gl_context.cc:357] GL version: 2.1 (2.1 Metal - 88.1), renderer: Apple M1\n",
      "W0000 00:00:1733642619.218984  295375 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1733642619.223036  295375 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed and saved outputs for test_publishable_list_3_122\n",
      "Processed and saved outputs for test_publishable_list_1_122\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0000 00:00:1733642619.366124  280999 gl_context.cc:357] GL version: 2.1 (2.1 Metal - 88.1), renderer: Apple M1\n",
      "W0000 00:00:1733642619.368962  295384 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1733642619.373255  295382 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1733642619.509085  280999 gl_context_nsgl.cc:104] Requested context not created, using queried context.\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Service \"kGpuService\", required by node facedetectionshortrangecpu__facedetectionshortrange__facedetection__ImageToTensorCalculator, was not provided and cannot be created: ; RET_CHECK failure (mediapipe/gpu/gl_context_nsgl.cc:116) context_Could not create an NSOpenGLContext",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[16], line 30\u001b[0m\n\u001b[1;32m     27\u001b[0m image_depth \u001b[38;5;241m=\u001b[39m preprocess_image(depth_path)\n\u001b[1;32m     28\u001b[0m image_roughness \u001b[38;5;241m=\u001b[39m preprocess_image(roughness_path)\n\u001b[0;32m---> 30\u001b[0m lip_mask, lip_binary_mask \u001b[38;5;241m=\u001b[39m \u001b[43mlip_segmentation\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimage_albedo\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     31\u001b[0m lip_mask_diffuse \u001b[38;5;241m=\u001b[39m cv2\u001b[38;5;241m.\u001b[39mbitwise_and(image_diffuse, image_diffuse, mask\u001b[38;5;241m=\u001b[39mlip_binary_mask)\n\u001b[1;32m     32\u001b[0m lip_mask_specular \u001b[38;5;241m=\u001b[39m cv2\u001b[38;5;241m.\u001b[39mbitwise_and(image_specular, image_specular, mask\u001b[38;5;241m=\u001b[39mlip_binary_mask)\n",
      "Cell \u001b[0;32mIn[5], line 15\u001b[0m, in \u001b[0;36mlip_segmentation\u001b[0;34m(image_rgb)\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mlip_segmentation\u001b[39m(image_rgb):\n\u001b[1;32m     14\u001b[0m     \u001b[38;5;66;03m# mp_face_mesh = mp.solutions.face_mesh\u001b[39;00m\n\u001b[0;32m---> 15\u001b[0m     face_mesh \u001b[38;5;241m=\u001b[39m \u001b[43mmp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msolutions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mface_mesh\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mFaceMesh\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     16\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstatic_image_mode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_num_faces\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrefine_landmarks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m     17\u001b[0m     results \u001b[38;5;241m=\u001b[39m face_mesh\u001b[38;5;241m.\u001b[39mprocess(image_rgb)\n\u001b[1;32m     19\u001b[0m     lip_indices \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m     20\u001b[0m         \u001b[38;5;241m61\u001b[39m, \u001b[38;5;241m146\u001b[39m, \u001b[38;5;241m91\u001b[39m, \u001b[38;5;241m181\u001b[39m, \u001b[38;5;241m84\u001b[39m, \u001b[38;5;241m17\u001b[39m, \u001b[38;5;241m314\u001b[39m, \u001b[38;5;241m405\u001b[39m, \u001b[38;5;241m321\u001b[39m, \u001b[38;5;241m375\u001b[39m,\n\u001b[1;32m     21\u001b[0m         \u001b[38;5;241m291\u001b[39m, \u001b[38;5;241m61\u001b[39m, \u001b[38;5;241m185\u001b[39m, \u001b[38;5;241m40\u001b[39m, \u001b[38;5;241m39\u001b[39m, \u001b[38;5;241m37\u001b[39m, \u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m267\u001b[39m, \u001b[38;5;241m269\u001b[39m, \u001b[38;5;241m270\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     24\u001b[0m         \u001b[38;5;241m311\u001b[39m, \u001b[38;5;241m310\u001b[39m, \u001b[38;5;241m415\u001b[39m, \u001b[38;5;241m308\u001b[39m\n\u001b[1;32m     25\u001b[0m     ]\n",
      "File \u001b[0;32m/opt/anaconda3/envs/l3d/lib/python3.9/site-packages/mediapipe/python/solutions/face_mesh.py:95\u001b[0m, in \u001b[0;36mFaceMesh.__init__\u001b[0;34m(self, static_image_mode, max_num_faces, refine_landmarks, min_detection_confidence, min_tracking_confidence)\u001b[0m\n\u001b[1;32m     70\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m     71\u001b[0m              static_image_mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m     72\u001b[0m              max_num_faces\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m,\n\u001b[1;32m     73\u001b[0m              refine_landmarks\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m     74\u001b[0m              min_detection_confidence\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.5\u001b[39m,\n\u001b[1;32m     75\u001b[0m              min_tracking_confidence\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.5\u001b[39m):\n\u001b[1;32m     76\u001b[0m \u001b[38;5;250m  \u001b[39m\u001b[38;5;124;03m\"\"\"Initializes a MediaPipe Face Mesh object.\u001b[39;00m\n\u001b[1;32m     77\u001b[0m \n\u001b[1;32m     78\u001b[0m \u001b[38;5;124;03m  Args:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     93\u001b[0m \u001b[38;5;124;03m      https://solutions.mediapipe.dev/face_mesh#min_tracking_confidence.\u001b[39;00m\n\u001b[1;32m     94\u001b[0m \u001b[38;5;124;03m  \"\"\"\u001b[39;00m\n\u001b[0;32m---> 95\u001b[0m   \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[1;32m     96\u001b[0m \u001b[43m      \u001b[49m\u001b[43mbinary_graph_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m_BINARYPB_FILE_PATH\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     97\u001b[0m \u001b[43m      \u001b[49m\u001b[43mside_inputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m{\u001b[49m\n\u001b[1;32m     98\u001b[0m \u001b[43m          \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mnum_faces\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_num_faces\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     99\u001b[0m \u001b[43m          \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mwith_attention\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mrefine_landmarks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    100\u001b[0m \u001b[43m          \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43muse_prev_landmarks\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mstatic_image_mode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    101\u001b[0m \u001b[43m      \u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    102\u001b[0m \u001b[43m      \u001b[49m\u001b[43mcalculator_params\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m{\u001b[49m\n\u001b[1;32m    103\u001b[0m \u001b[43m          \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mfacedetectionshortrangecpu__facedetectionshortrange__facedetection__TensorsToDetectionsCalculator.min_score_thresh\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m:\u001b[49m\n\u001b[1;32m    104\u001b[0m \u001b[43m              \u001b[49m\u001b[43mmin_detection_confidence\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    105\u001b[0m \u001b[43m          \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mfacelandmarkcpu__ThresholdingCalculator.threshold\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m:\u001b[49m\n\u001b[1;32m    106\u001b[0m \u001b[43m              \u001b[49m\u001b[43mmin_tracking_confidence\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    107\u001b[0m \u001b[43m      \u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    108\u001b[0m \u001b[43m      \u001b[49m\u001b[43moutputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mmulti_face_landmarks\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/l3d/lib/python3.9/site-packages/mediapipe/python/solution_base.py:263\u001b[0m, in \u001b[0;36mSolutionBase.__init__\u001b[0;34m(self, binary_graph_path, graph_config, calculator_params, graph_options, side_inputs, outputs, stream_type_hints)\u001b[0m\n\u001b[1;32m    257\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_graph\u001b[38;5;241m.\u001b[39mobserve_output_stream(stream_name, callback, \u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m    259\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_input_side_packets \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m    260\u001b[0m     name: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_make_packet(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_side_input_type_info[name], data)\n\u001b[1;32m    261\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m name, data \u001b[38;5;129;01min\u001b[39;00m (side_inputs \u001b[38;5;129;01mor\u001b[39;00m {})\u001b[38;5;241m.\u001b[39mitems()\n\u001b[1;32m    262\u001b[0m }\n\u001b[0;32m--> 263\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_graph\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstart_run\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_input_side_packets\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Service \"kGpuService\", required by node facedetectionshortrangecpu__facedetectionshortrange__facedetection__ImageToTensorCalculator, was not provided and cannot be created: ; RET_CHECK failure (mediapipe/gpu/gl_context_nsgl.cc:116) context_Could not create an NSOpenGLContext"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# Load CelebA Metadata\n",
    "# metadata_path = 'celeba-dataset-metadata.json'\n",
    "image_files = load_celeba_data(\"test_publishable_list\") # change this on different devices\n",
    "\n",
    "# Create output directory if it doesn't exist\n",
    "output_dir = 'lip-segmentation-outputs'\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# Process Each Image\n",
    "for image_path in image_files:\n",
    "    # Retrieve individual image layers\n",
    "    albedo_path = os.path.join(image_path, \"1_Albedo.exr\")\n",
    "    diffuse_path = os.path.join(image_path, \"1_Diffuse.exr\")\n",
    "    specular_path = os.path.join(image_path, \"1_Specular_naivelyscaled.exr\")\n",
    "    depth_path = os.path.join(image_path, \"1_Depth.exr\")\n",
    "    roughness_path = os.path.join(image_path, \"1_Roughness_map.exr\")   \n",
    "    if not os.path.exists(albedo_path) or not os.path.exists(diffuse_path) or not os.path.exists(specular_path) or \\\n",
    "        not os.path.exists(depth_path) or not os.path.exists(roughness_path):\n",
    "        print(\"Missing image path!\")\n",
    "        continue\n",
    "\n",
    "    image_albedo = preprocess_image(albedo_path)\n",
    "    image_diffuse = preprocess_image(diffuse_path)\n",
    "    image_specular = preprocess_image(specular_path)\n",
    "    image_depth = preprocess_image(depth_path)\n",
    "    image_roughness = preprocess_image(roughness_path)\n",
    "\n",
    "    lip_mask, lip_binary_mask = lip_segmentation(image_albedo)\n",
    "    lip_mask_diffuse = cv2.bitwise_and(image_diffuse, image_diffuse, mask=lip_binary_mask)\n",
    "    lip_mask_specular = cv2.bitwise_and(image_specular, image_specular, mask=lip_binary_mask)\n",
    "    lip_mask_depth = cv2.bitwise_and(image_depth, image_depth, mask=lip_binary_mask)\n",
    "    lip_mask_roughness = cv2.bitwise_and(image_roughness, image_roughness, mask=lip_binary_mask)\n",
    "    \n",
    "    # Apply lipstick color to albedo image and generate albedo mask\n",
    "    lipstick_color = np.array([255, 0, 0])\n",
    "    blended_albedo_mask = blend_albedo(image_albedo, lip_binary_mask, lipstick_color, blend_factor=0.25)\n",
    "\n",
    "    # Enhance specular highlights using roughness map\n",
    "    adjusted_specular_mask = adjust_specular_with_roughness(lip_mask_specular, lip_mask_roughness, intensity=0.3) \n",
    "\n",
    "    # Composite diffuse and specular masks with the base (albedo) image\n",
    "    final_image = composite_final_image(image_albedo, blended_albedo_mask, lip_mask_diffuse, adjusted_specular_mask)\n",
    "\n",
    "    # Save composited image and lip masks\n",
    "    base_name = os.path.splitext(os.path.basename(image_path))[0]\n",
    "    output_path = os.path.join(output_dir, f'{base_name}')\n",
    "    os.makedirs(output_path, exist_ok=True)\n",
    "    cv2.imwrite(os.path.join(output_path, 'lip_mask.png'), cv2.cvtColor(lip_mask, cv2.COLOR_RGB2BGR))\n",
    "    cv2.imwrite(os.path.join(output_path, 'binary_mask.png'), lip_binary_mask)\n",
    "    final_image_float = final_image.astype(np.float32) / 255.0\n",
    "    cv2.imwrite(os.path.join(output_path, 'final_adjusted.exr'), cv2.cvtColor(final_image_float, cv2.COLOR_RGB2BGR))\n",
    "\n",
    "    print(f'Processed and saved outputs for {base_name}')\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Next Step - Parallelize the processing with a dataloader type multiprocessing. Perhaps using image transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
